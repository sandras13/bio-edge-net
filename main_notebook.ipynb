{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VQMOv428DfA",
        "outputId": "d0e7512b-c75a-4048-b179-69a786c3d2e8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bf06Dl-pPoWP"
      },
      "outputs": [],
      "source": [
        "pip install -q tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5bXIvsnVQm0v"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/deep_learning_quantized')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import architectures\n",
        "import network_training\n",
        "import preprocessing\n",
        "from scipy import fft\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F0GDzAhFrvHV"
      },
      "outputs": [],
      "source": [
        "def read_data_from_csv(csv_file_name):\n",
        "      data = pd.read_csv(csv_file_name)\n",
        "      x_data = data.iloc[:, :-2].values\n",
        "      y_data = data.iloc[:, -2].values\n",
        "      subj = data.iloc[:, -1].values\n",
        "\n",
        "      return x_data, y_data, subj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eCyTR0urJoJw"
      },
      "outputs": [],
      "source": [
        "dataset_id = 1 # 1 - AffectiveROAD, 2 - PPG_ACC\n",
        "data_used = 1 # 1 - PPG only, 2 - ACC only, 3 - both ACC & PPG\n",
        "quant_type = 1 # 1 - 8b integer, 2 - 16b float, 3 - dynamic range\n",
        "\n",
        "if dataset_id == 1:\n",
        "  csv_file_name = '/content/drive/MyDrive/deep_learning_quantized/multimodal_data.csv'\n",
        "  labels = ('low', 'medium', 'high')\n",
        "  avg = True\n",
        "  fs = 64\n",
        "  step_size = 8\n",
        "\n",
        "elif dataset_id == 2:\n",
        "  csv_file_name = '/content/drive/MyDrive/deep_learning_quantized/combined_data.csv'\n",
        "  labels = ('rest', 'squat', 'step')\n",
        "  avg = False\n",
        "  fs = 400\n",
        "  step_size = 32\n",
        "\n",
        "features, targets, subj_data = read_data_from_csv(csv_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0delxoDwp0D",
        "outputId": "a80c8496-1d9d-4fff-cc47-9c9e11d2f9fa"
      },
      "outputs": [],
      "source": [
        "if data_used == 1: features = features[:, 0].reshape(-1, 1)\n",
        "elif data_used == 2: features = features[:, 1:4]\n",
        "else: features = features[:, 0:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wTv6Q3Ubawlw"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    \"seed\" : 256,\n",
        "    \"learning_rate\" : 0.002,\n",
        "    \"weight_decay\" : 1e-6,\n",
        "    \"step_size\" : 3,\n",
        "    \"gamma\" : 0.8,\n",
        "    \"batch_size\" : 128,\n",
        "    \"epochs\" : 3,\n",
        "    \"num_resblocks\" : 1,\n",
        "}\n",
        "\n",
        "params[\"window_size\"] = 256\n",
        "params[\"overlap\"] = params[\"window_size\"] * 7//8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzwYFHnv_J4I",
        "outputId": "0dc600cf-eb84-4b14-f833-f567fbb9e1d4"
      },
      "outputs": [],
      "source": [
        "# The data can be standarzided, but this was not used in the final version\n",
        "# scaled_data = preprocessing.scale_data(features, targets)\n",
        "\n",
        "scaled_data = features\n",
        "targ = targets\n",
        "\n",
        "# We take every N-th sample for both the features and the targets\n",
        "scaled_data = scaled_data[::step_size]\n",
        "targ = targets[::step_size]\n",
        "print(scaled_data.shape, targ.shape)\n",
        "\n",
        "sliding_X_data, sliding_y_data = preprocessing.apply_sliding_window(scaled_data, targ, subj_data, params[\"window_size\"], params[\"overlap\"], avg)\n",
        "\n",
        "X_data = sliding_X_data.astype(np.float32)\n",
        "y_data = sliding_y_data.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ItuhNLcn_3VR"
      },
      "outputs": [],
      "source": [
        "params[\"num_channels\"] = X_data.shape[2]\n",
        "params[\"num_classes\"] = len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "j0NGC9niUIAf"
      },
      "outputs": [],
      "source": [
        "def create_functional_resnet(input_shape, num_classes, num_resblocks):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = layers.Conv2D(4, kernel_size=(3, 1), strides=(2, 1), padding='same', use_bias=False)(inputs)\n",
        "\n",
        "    x = layers.Conv2D(16, kernel_size=(5, 1), strides=(4, 1), padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    x = layers.Conv2D(32, kernel_size=(3, 1), strides=(2, 1), padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    x = layers.Conv2D(64, kernel_size=(3, 1), strides=(2, 1), padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    for _ in range(num_resblocks):\n",
        "        shortcut = x\n",
        "        x = layers.Conv2D(int(64*2), kernel_size=(1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.ReLU()(x)\n",
        "\n",
        "        x = layers.DepthwiseConv2D(kernel_size=(3, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.ReLU()(x)\n",
        "\n",
        "        x = layers.Conv2D(64, kernel_size=(1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.ReLU()(x)\n",
        "\n",
        "        x = layers.Add()([x, shortcut])\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xkqsfrA1Ut8I"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "import keras.optimizers\n",
        "from keras.metrics import Precision, Recall\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import time\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "def train_model(params, X_train, y_train, X_test, y_test, pruning = False):\n",
        "    def lr_schedule(epoch, lr):\n",
        "      if epoch % params['step_size'] == 0 and epoch != 0:\n",
        "        return lr * params['gamma']\n",
        "      return lr\n",
        "\n",
        "    tf.profiler.experimental.start('logdir')\n",
        "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "    optimizer = keras.optimizers.AdamW(learning_rate=params['learning_rate'])\n",
        "\n",
        "    precision = Precision()\n",
        "    recall = Recall()\n",
        "\n",
        "    def f1_metric(y_true, y_pred):\n",
        "        precision_value = precision(y_true, y_pred)\n",
        "        recall_value = recall(y_true, y_pred)\n",
        "        return 2 * ((precision_value * recall_value) / (precision_value + recall_value + 1e-7))\n",
        "\n",
        "    input_shape = X_train.shape[1:]\n",
        "    model = create_functional_resnet(input_shape, params['num_classes'], params['num_resblocks'])\n",
        "    print(model.summary())\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    if pruning:\n",
        "      model_for_pruning = network_training.get_pruned_model(model, X_data, params)\n",
        "\n",
        "      callbacks = [\n",
        "          tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "          lr_scheduler\n",
        "        ]\n",
        "\n",
        "      model_for_pruning.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "      model_for_pruning.fit(X_train, y_train, batch_size=params['batch_size'],\n",
        "                epochs=params['epochs'], verbose=1, validation_data=(X_test, y_test),\n",
        "                callbacks=callbacks)\n",
        "\n",
        "    else:\n",
        "      model.fit(X_train, y_train, batch_size=params['batch_size'],\n",
        "              epochs=params['epochs'], verbose=1, validation_data=(X_test, y_test),\n",
        "              callbacks=[lr_scheduler])\n",
        "      model_for_pruning = model;\n",
        "\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "    non_quantized_time = time.time() - start_time\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    precision = precision_score(y_true_classes, y_pred_classes, average='macro')\n",
        "    recall = recall_score(y_true_classes, y_pred_classes, average='macro')\n",
        "    f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
        "\n",
        "    tf.profiler.experimental.stop()\n",
        "\n",
        "    return model, accuracy, f1, precision, recall, non_quantized_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1CSNhfY2Qnhx"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def training_loop(X_data, y_data, params, crossvalid = False, quant_type = 1):\n",
        "  if crossvalid:\n",
        "    num_folds = 5\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True, random_state = params[\"seed\"])\n",
        "    data_splits = [(X_data[train_ind], X_data[val_ind], y_data[train_ind], y_data[val_ind]) for train_ind, val_ind in kf.split(X_data)]\n",
        "  else:\n",
        "    num_folds = 1\n",
        "    X_train, X_val, y_train, y_val = network_training.split_data(X_data, y_data, train_size=0.8)\n",
        "    data_splits = [(X_train, X_val, y_train, y_val)]\n",
        "\n",
        "  total_accuracy = []\n",
        "  total_f1_score = []\n",
        "  total_precision = []\n",
        "  total_recall = []\n",
        "  total_non_quantized_time = []\n",
        "\n",
        "  total_accuracy_quant = []\n",
        "  total_f1_score_quant = []\n",
        "  total_precision_quant = []\n",
        "  total_recall_quant = []\n",
        "  total_quantized_time = []\n",
        "\n",
        "  for fold, (X_train_fold, X_val_fold, y_train_fold, y_val_fold) in enumerate(data_splits):\n",
        "      print(f\"Fold {fold+1}/{num_folds}\")\n",
        "\n",
        "      y_train_fold = keras.utils.to_categorical(y_train_fold, num_classes=params['num_classes'])\n",
        "      y_val_fold = keras.utils.to_categorical(y_val_fold, num_classes=params['num_classes'])\n",
        "\n",
        "      X_train_fold = np.expand_dims(X_train_fold, axis=-1)\n",
        "      X_val_fold = np.expand_dims(X_val_fold, axis=-1)\n",
        "\n",
        "      model, accuracy, f1_score, precision, recall, non_quantized_time = train_model(params, X_train_fold, y_train_fold, X_val_fold, y_val_fold, pruning=False)\n",
        "\n",
        "      tflite_quant_model = network_training.get_quantized_model(model, X_train_fold, quant_type)\n",
        "      interpreter = network_training.get_tflite_interpreter(tflite_quant_model)\n",
        "      accuracy_quant, f1_score_quant, precision_quant, recall_quant, quantized_time = network_training.evaluate_quantized_metrics(interpreter, X_val_fold, y_val_fold)\n",
        "\n",
        "      start_time = time.time()\n",
        "      score = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
        "      non_quantized_time = time.time() - start_time\n",
        "\n",
        "      print(f\"\"\"Non-quantized: acc - {accuracy}, f1 - {f1_score}, prec - {precision}, rec - {recall}\"\"\")\n",
        "      print(f\"\"\"Quantized: acc - {accuracy_quant}, f1 - {f1_score_quant}, prec - {precision_quant}, rec - {quantized_time}\"\"\")\n",
        "      print(f\"\"\"Non-quantized time: {non_quantized_time}; Quantized time: {quantized_time} \"\"\")\n",
        "\n",
        "      if fold == 0:\n",
        "        print(model.summary())\n",
        "        non_quantized_model_size, quantized_model_size =  network_training.compare_model_sizes(tflite_quant_model, model)\n",
        "        print(f\"\"\"Non-quantized size: {non_quantized_model_size}; Quantized size: {quantized_model_size} KB \"\"\")\n",
        "\n",
        "      total_accuracy.append(accuracy)\n",
        "      total_f1_score.append(f1_score)\n",
        "      total_precision.append(precision)\n",
        "      total_recall.append(recall)\n",
        "      total_non_quantized_time.append(non_quantized_time)\n",
        "\n",
        "      total_accuracy_quant.append(accuracy_quant)\n",
        "      total_f1_score_quant.append(f1_score_quant)\n",
        "      total_precision_quant.append(precision_quant)\n",
        "      total_recall_quant.append(recall_quant)\n",
        "      total_quantized_time.append(quantized_time)\n",
        "\n",
        "  mean_accuracies = np.mean(total_accuracy)\n",
        "  mean_f1_scores = np.mean(total_f1_score)\n",
        "  mean_precisions = np.mean(total_precision)\n",
        "  mean_recalls = np.mean(total_recall)\n",
        "  mean_times = np.mean(total_non_quantized_time)\n",
        "\n",
        "  mean_accuracies_quant = np.mean(total_accuracy_quant)\n",
        "  mean_f1_scores_quant = np.mean(total_f1_score_quant)\n",
        "  mean_precisions_quant = np.mean(total_precision_quant)\n",
        "  mean_recalls_quant = np.mean(total_recall_quant)\n",
        "  mean_times_quant = np.mean(total_quantized_time)\n",
        "  print(f\"\"\"Non-quantized:  acc - {mean_accuracies*100:.2f} F1 - {mean_f1_scores*100:.2f}\n",
        "        prec - {mean_precisions*100:.2f} rec - {mean_recalls*100:.2f}\n",
        "        time [ms] - {mean_times*1000:.2f} \"\"\")\n",
        "\n",
        "  print(f\"\"\"Quantized:  acc - {mean_accuracies_quant*100:.2f} F1 - {mean_f1_scores_quant*100:.2f}\n",
        "        prec - {mean_precisions_quant*100:.2f} rec - {mean_recalls_quant*100:.2f}\n",
        "        time [ms] - {mean_times_quant*1000:.2f} \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VOMKMmuvQ-YT",
        "outputId": "b83b196c-7305-4ddb-d956-64cfa4ef0cbb"
      },
      "outputs": [],
      "source": [
        "import keras.utils\n",
        "\n",
        "keras.utils.set_random_seed(params[\"seed\"])\n",
        "np.random.seed(params[\"seed\"])\n",
        "training_loop(X_data, y_data, params, False, quant_type)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
